%----------------------------------------------------------------------------
\chapter{Megvalósítás}
%----------------------------------------------------------------------------


Ebben a fejezetben az előzőekben bemutatott rendszer megvalósítását mutatom be lépésről lépésre, melyek \aref{fig:of-method}. ábrában közölt sorrendet követik. A beszámoló során kitérek az alkalmazott jelentősebb OpenCV eljárásokra és a választott paraméterekre, amennyiben azok további magyarázatot igényelnek. 


%----------------------------------------------------------------------------
\section{Alapok}
%----------------------------------------------------------------------------



Az OpenCV igen hasznos keretrendszer, rengeteg gyakran használt algoritmus implementációját tartalmazza, viszont felépítését tekintve procedurális. Jelen feladatom megoldása során törekedtem az átlátható és jól struktúrált kód kialakítására, így a logikát jól körülhatárolt osztályokba szerveztem.

OpenCV-ben a legtöbb adatot egy mátrix (\texttt{cv::Mat}) adattípus reprezentál, ide értve a matematikai értelemben vett mátrixokat és a képeket is. Egy ilyen mátrix lényegében egy kétdimenziós tömb, melynek elemei lehetnek skalárok, de több-dimenziós vektorok is (több csatornás).

Elsőnek a \texttt{Camera} osztály, és annak konkrét implementációi készültek el, elfedve azt, hogy éppen a valódi kamerából kérünk le képkockákat, vagy fájlból olvassuk ki azokat. Ebből adódóan a \texttt{RealCamera} lényegében becsomagolja az OpenCV-s \texttt{VideoCapture} osztályt, és a \texttt{Camera} absztakt osztály közös interfészt nyújt a fájlból történő olvasáshoz is a \texttt{FakeCamera} számára. Ez főleg a tesztelés során volt hasznos, hogy egy adott jelenetet elég volt egyszer felvenni, és utána azt tudtam bemenetként használni. Az osztálydiagram \aref{fig:cd:camera}. ábrán látható.

A \texttt{cameraMatrix} attribútum jelöli a \textit{kamera-mátrix}ot és a \texttt{distCoeffs} attribútum a torzítási együtthatókat (lásd \aref{sec:pinhole}. szekció). Mivel ezek egy kamerára nézve időben állandók, ezért csak egyszer kell őket meghatározni. A \texttt{readCalibration()} metódus szolgál ezek beolvasására egy külső fájlból. \begin{comment} Miután rendelkezésre állnak ezek a paraméterek, akkor a \texttt{readUndistorted()} metódus segítségével olvashatok be rektifikált képet. \end{comment} A másik három metódus megfelel az azonos nevű metódusoknak a \texttt{cv::VideoCapture} osztályban \cite{cv_video}, ahol a \texttt{grab()} egy képkockát szerez az eszköztől, de nem olvassa (dekódolja) ki, míg a \texttt{retrieve()} ezt teszi. Ezt a kombinációt több kamerás rendszernél célszerű használni, úgy, hogy először mindegyik kamerán meghívjuk a \texttt{grab()}-et, majd utána lekérjük a képeket (amely művelet időigényes). Ezzel a módszerrel érhető el, hogy időben a lehető legközelebb legyenek a különböző kamerákból lekért képkockák egymáshoz. A \texttt{read()} a kettőt kombinálja kényelmi szempontból.

\begin{figure}[t]
\centering

\begin{tikzpicture} 

\begin{abstractclass}[text width=7 cm]{Camera}{0, 0}
\attribute   {+ cameraMatrix : Mat}
\attribute   {+ distCoeffs : Mat}

\operation   {+ readCalibration(fileName) : void}
%\operation   {+ readUndistorted(outputImg) : void}
\operation[0]{+ grab() : void}
\operation[0]{+ retrieve(outputImg) : void}
\operation[0]{+ read(outputImg) : void}
\end{abstractclass}

\begin{class}[text width=5 cm]{RealCamera}{-4, -5.5}
\inherit{Camera}
\operation{+ grab() : void}
\operation{+ retrieve(outputImg) : void}
\operation{+ read(outputImg) : void}
\end{class}

\begin{class}[text width=5 cm]{DummyCamera}{4, -5.5}
\inherit{Camera}
\operation{+ grab() : void}
\operation{+ retrieve(outputImg) : void}
\operation{+ read(outputImg) : void}
\end{class}

\end{tikzpicture}

\caption{Osztályok a kamerához kezeléséhez \label{fig:cd:camera}}
\end{figure}


\section{Kalibráció}

Elsőként a kamerák belső paramétereit határoztam meg. A módszert \aref{sec:pinhole}. szekcióban mutattam be, a következőkben ennek megvalósítását tárgyalom. Ehhez készült egy segédosztály \texttt{Calibration} névvel, melynek feladata, hogy a szükséges információk alapján meghatározza a kamera-mátrixot és a torzítási együtthatókat és kiírja ezeket egy fájlba, hogy azt vissza lehessen olvasni.

\begin{figure}[tbh]
\centering

\begin{tikzpicture} 

\begin{class}[text width=7 cm]{Calibration}{0, 0}
\attribute{- camera : Ptr<Camera>}
\attribute{- corners : vector<vector<Point2f>{}>}
\attribute{- cameraMatrix : Mat}
\attribute{- distCoeffs : Mat}

\operation{+ Calibration(camera)}
\operation{+ acquireFrame() : bool}
\operation{+ calibrate() : void}
\operation{+ save(fileName) : void}
\end{class}

\end{tikzpicture}

\caption{\texttt{Calibration} osztály \label{fig:cd:calibration}}
\end{figure}

Konstruktorban át kell neki adni a kamerára vonatkozó pointert, amitől a képeket kell majd lekérnie. Az \texttt{acquireFrame()} metódus szerepe, hogy a kamera aktuális képkockáját megszerezze, megkeresse a képen a sakktáblát, és a sakktábla sarokpontjaihoz tartozó koordinátákat a \texttt{corners} listához fűzze. Visszatérési értékben jelzi, hogy az adott képen sikeres volt-e a detekció. Belsőleg a \texttt{cv::findChessboardCorners()} OpenCV-s függvényt hívja, amelynek átadva egy fekete-fehér képet, megkapható a képen látható sakktábla sarokpontjainak képpontjai. Kellő számú (10-15) képkocka után a \texttt{calibrate()} metódus segítségével a kérdéses két mátrix (\texttt{cameraMatrix}, \texttt{distCoeffs}) kiszámolható. Itt a \texttt{cv::calibrateCamera()} függvényt hívtam segítségül, melynek két fontos bemeneti paraméterét emelem ki: a sarokpontok valóvilágbeli koordinátái, és a képeken detektált képpontjai sorfolytonosan (\texttt{corners}). A valóvilágbeli $(x, y, z)$ koordinátákat az egyszerűség kedvéért úgy választottam, hogy $z \equiv 0$, és $x$ valamint $y$ egész számok úgy, hogy a sakktábla bal felső sarka $(0, 0, 0)$, jobb alsó sarka pedig -- $9\times 6$-os sakktáblát használva -- $(9, 6, 0)$. A \texttt{save()} metódus a kiszámolt paramétereket menti el a megadott fájlnévvel olyan formátumban, amit a \texttt{Camera::readCalibration()} vissza tud olvasni.


\subsection{Kamerák pozíciójának meghatározása világkoordinátákban}


Rögzített kamerák révén lehetőségem adódik, hogy előre meghatározzam a kamerák pozícióját és nézőpontjuk irányát. Erre egy kalibrációs objektumot használok, szintén egy sakktáblát. Amennyiben megadom a sakktábla sarokpontjainak koordinátáit az előbbiekkel egyező módon, akkor a sakktábla rögzítésével a térben, a koordinátarendszert is rögzítem. 

Az OpenCV-ben erre a célra van a \texttt{solvePnP} függvény, mely 3D-2D pont-összerendelésekből kiszámolja a forgatási és eltolási vektort, amik együttesen megadják a transzformációt a model-koordinátarendszerből a kamera koordinátarendszerébe. Ezen funkciót a \texttt{CameraPoseCalculator} osztályba ágyaztam, a két vektort pedig a \texttt{CameraPose} perzisztálható osztályba csomagoltam, lásd \aref{fig:cd:pose}. ábra.

\begin{figure}[tbh]
\centering

\begin{tikzpicture} 

\begin{class}[text width=5.3 cm]{CameraPoseCalculator}{-8.5, -0.50}
\attribute{- cam : Ptr<Camera>}

\operation{+ CameraPoseCalculator(cam)}
\operation{+ calculator() : bool}
\end{class}

\begin{class}[text width=5 cm]{CameraPose}{0, 0}
\attribute{+ rvec : Mat}
\attribute{+ tvec : Mat}

\operation{+ save(fileName) : void}
\operation{+ load(fileName) : void}
\operation{+ getRT() : Matx34d}
\end{class}

\aggregation{CameraPoseCalculator}{cameraPose~~~~~~~~~~}{1}{CameraPose}

\end{tikzpicture}

\caption{\texttt{CameraPoseCalculator} és \texttt{CameraPose} osztály \label{fig:cd:pose}}
\end{figure}

A \texttt{CameraPoseCalculator} osztály megkapja konstruktor argumentumaként annak a kamerára mutató pointerét, amelynek a külső paramétereit (forgatási és eltolási vektor, lásd \ref{sec:pinhole}. szekció vége) ki kell számolnia. A konkrét művelet végrehajtásáért a \texttt{calculator()} metódus felelős, amely a kamerától lekér egy képkockát, megkeresi rajta a sakktáblát, majd meghívja a \texttt{cv::solvePnP()} függvényt. Visszatérési értékben jelzi, hogy sikeres volt-e a detekció, ha igen, akkor lekérhető tőle a \texttt{CameraPose} példány. Ez utóbbi a \texttt{save()} és \texttt{load()} metódusokkal elmenthető és visszatölthető, így ameddig a kamerát nem mozgatjuk el, ez újra felhasználható. A \texttt{getRT()} metódus a forgatási vektort forgatási mátrix-szá alakítja (Rodrigues-féle forgatási formulával \cite{camera-calib-3d}) és összefűzi azt az eltolási vektorral egy $3\times 4$-es $\Big(\,\mathbf{R}\,|\,\mathbf{t}\,\Big)$ forgatás-eltolás mátrixba.

A kamera külső paramétereinek meghatározása után már minden információ adott, hogy 3D-s pontok 2D-s vetületeit meg tudjam határozni a \texttt{cv::projectPoints()} függvény felhasználásával. \Aref{fig:pose}. ábrán látható egy, a sakktábla síkjába rajzolt négyzetrács, melynek egyik jelölt pontja a világ-koordinátarendszer origója.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/pose0_180.png}
	\caption{Bal oldali kamera}
  \end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/pose1_180.png}
	\caption{Jobb oldali kamera}
  \end{subfigure}
\caption{Világ-koordinátarendszer jelölése a képeken \label{fig:pose}}
\end{figure}

%----------------------------------------------------------------------------
\section{Objektum detektálás}
%----------------------------------------------------------------------------

A következőkben az objektumok detektálásáról lesz szó. Először bemutatok két megközelítést az előtér maszk meghatározásához, ami lényegében kijelöli a két képen látható mozgó részeket. Ezt követően a maszkokat szegmentálom különálló \textit{blob}okra (egy objektumhoz tartozó egybefüggő rész a képen), és ezeket a két képen egymásnak megfeleltetem, hogy azonosítsam az egy objektumhoz tartozó képrészleteket.

    %----------------------------------------------------------------------------
    \subsection{Előtér maszk meghatározása}
    %----------------------------------------------------------------------------


\subsubsection{Előtér-háttér szegmentáció}

\Aref{sec:obj_detection}. szekcióban leírtam a mozgó objektumok detektálásának egy lehetséges megközelítését. A lényege, hogy egy háttér modellt építünk, ami alapján mindig aktuálisan lekérhető az előtérhez tartozó maszk, ami kijelöli a mozgó objektumokat. OpenCV-ben ezt valósítja meg a \texttt{BackgroundSubtractorMOG2} osztály \cite{opencv-mog}. Példányosítás után a modell építése, és az aktuális maszk kinyerése az \texttt{apply} metódussal történik. \Aref{fig:my_mog2}. ábrán látható a kinyerhető maszkra egy példa.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/image230.png}
	\caption{Statikus kép -- ,,háttér''}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/image343.png}
	\caption{Mozgó képsor egy képe}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/mask343.png}
	\caption{Kapott előtér maszk}
  \end{subfigure}
\caption{Példa az előtér maszkra \label{fig:my_mog2}}
\end{figure}

Megfigyelhető, hogy a maszk nem bináris, hanem háromértékű: azt is tartalmazza, amit az algoritmus árnyékként detektál. A zajt a maszkon az erózió-dilatáció morfológiai módszerek segítségével tudjuk csökkenteni. Előbbi a kisméretű zajokat tünteti el, utóbbi pedig a lyukakat szünteti meg. OpenCV-ben ezek implementációi a \texttt{dilate} és \texttt{erode} függvények. Előbbi az adott pixelt a környezetében (amit egy kernel ír le) lévő maximális, míg utóbbi a minimális értékkel helyettesít. Én egy erózió-dilatáció-erózió lépéssorozatot használtam, amely egy morfológiai nyitás és zárás egymásutánja \cite{kepfeldolg-modszerek}. Ennek az eredményét \aref{fig:erosion_dilation}. ábra mutatja be.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/mask343.png}
	\caption{Eredeti maszk}
  \end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/mask343_fixed.png}
	\caption{Árnyékok kivétele és zajcsökkentés után}
  \end{subfigure}
\caption{Előtér maszk zajmentesítése \label{fig:erosion_dilation}}
\end{figure}

Megfigyelhető, hogy a bal oldali dobozhoz tartozó maszk annyira zajos volt, hogy a nagy lyuk nem szűnt meg, míg a kevésbé zajos jobb oldali doboz rendben megmaradt. A maszkot alkalmazva az eredeti képre \aref{fig:mask_applied}. ábrán látható, hogy egy bizonyos hibahatáron belül sikeresnek tekinthető a mozgó részlet kijelölése.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/image343.png}
	\caption{Eredeti kép}
  \end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/mask343_applied.png}
	\caption{A detektált előtér}
  \end{subfigure}
\caption{Előtér maszk alkalmazás az eredeti képre \label{fig:mask_applied}}
\end{figure}

    %----------------------------------------------------------------------------
    \subsubsection{Előtér meghatározása optikai folyamokkal \label{sec:of-mask}}
    %----------------------------------------------------------------------------
    
Másik megközelítésem során már az előtér meghatározásához is az optikai folyamokat hívtam segítségül. 

\Aref{chapter2}. fejezetben bemutattam a sűrű optikai folyamok meghatározására Gunner Farnebäck módszerét. OpenCV-ben ezt a \texttt{cv::calcOpticalFlowFarneback()} \cite{opencv-mog} függvény valósítja meg. Az algoritmus szempontjából fontos paraméterei közül kiemelném a következőket:

\begin{enumerate}
\item \texttt{pyr\_scale} -- a már említett piramis-módszerhez kapcsolódó paraméter: azt definiálja, hogy rétegenként a következő réteg hányszorosa az előzőnek (ennek segítségével a nagy elmozdulások kisebbek lesznek a kisebb rétegeken)
\item \texttt{levels} -- piramis rétegeinek a száma, 1-nél csak az eredeti képet használja, továbbiak nélkül
\item \texttt{winsize} -- az ablak méret, amit a mintavételezéshez használ
\item \texttt{iterations} -- iterációk száma minden piramis rétegen
\end{enumerate}

Tekintve, hogy nekünk csak az a fontos, hogy a lehető leggyorsabban számoljunk elmozdulásvektorokat, a paramétereket a következőknek megfelelően állítottam be: ne építsünk piramist (\texttt{levels = 1}, így \texttt{pyr\_scale} beállítása lényegtelen), kicsi ablakmérettel dolgozzunk ($3\times 3$, tehát \texttt{winsize = 3}), valamint rétegenként csak egy iteráció legyen (\texttt{iterations = 1}).

A kiszámolt vektormezőből a maszkokat úgy kaptam, hogy minden, egységnél hosszabb (tehát legalább 1 pixelnyi a becsült mozgása) vektor végpontját megjelöltem. Ezáltal lényegében azon részeket határoztam meg, ahova éppen a pontok mozdultak, ez pedig pontosan azon részei a képnek, ahol az adott képkockán az előtérben mozgó objektumokat várjuk. Az előzőekhez hasonlóan a maszkon itt is végeztem apró utófeldolgozást a dilatáció és erózió morfológiai műveletek segítségével. Az eredményt \aref{fig:of_mask}. ábra mutatja be. Megfigyelhető, hogy a jól textúrázott részeken a maszk nagyon pontos, a textúrázatlanokon viszont a pontok statikus pontoknak tűnnek.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/frame_ofmask_101.png}
	\caption{2. képkocka}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/frame_ofmask_104.png}
	\caption{3. képkocka}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/frame_ofmask_107.png}
	\caption{4. képkocka}
  \end{subfigure}\\
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/mask_ofmask_101.png}
	\caption{2. maszk}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/mask_ofmask_104.png}
	\caption{3. maszk}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=137pt]{figures/mask_ofmask_107.png}
	\caption{4. maszk}
  \end{subfigure}
\caption{Egy jelenet 3 képkockája és ezekből az optikai folyamok felhasználásával kapott maszkok (már zajcsökkentés után) \label{fig:of_mask}}
\end{figure}


A bemutatott két eljáráshoz tartozó osztálydiagramot \aref{fig:cd:fg-mask-calc}. ábra mutatja be. \texttt{ForegroundMaskCalculator} osztály lényegében egy közös interfészt nyújt csak, valamint egy \texttt{morph()} metódust tartalmaz, mely az említett morfológiai műveleteket végzi el a maszkon. A \texttt{MOG2ForegroundMaskCalculator} az OpenCV-s \texttt{BackgroundSubtractorMOG2} osztály egy példányát csomagolja be, ami minden új képkockát megkap, és visszaadja a maszkot, az algoritmus által árnyéknak jelölt részeket a \texttt{removeShadows()} távolítja el. \texttt{OFForegroundMaskCalculator} pedig eltárolja az előző képkockát, és ebből meg az éppen aktuális képkockából \texttt{calcOpticalFlowFarneback()} segítségével a lehető leggyorsabban meghatározza a képkockákra az optikai folyamot, melyből aztán az előbbiekben ismertetett módszerrel a \texttt{getMaskFromFlow()} adja vissza a maszkot.

\begin{figure}[tbh]
\centering

\begin{tikzpicture} 

\begin{abstractclass}[text width=5.3 cm]{ForegroundMaskCalculator}{0, 0}

\operation   {\# morph(mask) : void}
\operation[0]{+ calculate(nextFrame) : Mat}

\end{abstractclass}

\begin{class}[text width=6.3 cm]{MOG2ForegroundMaskCalculator}{-3.8, -3.5}
\inherit{ForegroundMaskCalculator}
\attribute{- sub : BackgroundSubtractorMOG2}
\operation{- removeShadows(mask): void}

\operation{+ calculate(nextFrame) : Mat}
\end{class}

\begin{class}[text width=6.3 cm]{OFForegroundMaskCalculator}{3.8, -3.5}
\inherit{ForegroundMaskCalculator}
\attribute{- prevFrame : Mat}
\operation{- getMaskFromFlow(flow) : Mat}

\operation{+ calculate(nextFrame) : Mat}
\end{class}

\end{tikzpicture}

\caption{Előtér maszk meghatározására szolgáló implementációk \label{fig:cd:fg-mask-calc}}
\end{figure}

%----------------------------------------------------------------------------
\subsection{Egyetlen objektum detektálása}
%----------------------------------------------------------------------------

Kezdetben azzal az egyszerűsítéssel éltem, hogy a képen csak egyetlen mozgó objektumot detektálok, mégpedig azt, amelyik a legnagyobb részt foglalja el a képeken. Ez nagyban egyszerűsítette a problémát, mert ahogy majd a következő szekcióban bemutatom, az optikai folyam meghatározásához szükség van a képeken látható \textit{blob}ok párosítására a kamerák képein. Mivel összesen egy blobot jelölök ki a képeken, ezek párosítása triviális, és nagy valószínűséggel ugyanazon objektumhoz tartoznak.

Ehhez először szükség van az összefüggő komponensek kiválasztására. Két módszer kínálkozik erre OpenCV-ben; az egyik a \texttt{findContours()} függvény, ami megkeresi a képen látható kontúrokat (paraméterezhető, hogy csak a legkülsőbbeket találja meg, a belső kontúrokat nem), a másik pedig a a \texttt{connectedComponentsWithStats()}, mely a 3.0-s verziótól érhető el. Az első abban különbözik az utóbbitól, hogy a kapott kontúrt felhasználva megkaphatjuk a belső területet (lyukak nélkül), míg utóbbi lényegében a maszk pixeleit címkézi fel a komponenseknek megfelelően. Ezért én az előbbi használata mellett döntöttem. Miután meghatároztam a kontúrokat, ezek területeit a \texttt{cv::contourArea()} függvénnyel számoltam ki. Végül maximum kiválasztással a legnagyobb területtel rendelkező maszk mellett döntöttem. Mindkét képre elvégezve ezt, megkaptam az egyik és másik képen a legnagyobb objektumhoz tartozó 1-1 maszkot. Az algoritmus eredménye \aref{fig:single-obj}. ábrán látható.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/mask_ofmask_104.png}
	\caption{Eredeti maszk}
  \end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/mask_ofmask_104_selected.png}
	\caption{Detektált blobok, fehérrel jelölve a legnagyobb}
  \end{subfigure}
\caption{Legnagyobb területű blob kijelölése \label{fig:single-obj}}
\end{figure}

%----------------------------------------------------------------------------
\subsection{Több objektum detektálása, párosítás a kamerák képein}
%----------------------------------------------------------------------------

A következő feladat, amit meg kellett oldanom, az a több objektum együttes detektálása volt úgy, hogy a két képen az egymásnak megfelelő blobokat párosítom. Ehhez jellegzetes pontokat, és ezekhez tartozó leírókat kerestem, illetve számoltam ki a képi információk alapján.

A szakirodalomban több algoritmust is találhatunk, mely vagy a jellegzetes pontok (\textit{feature}) detektálásában (pl. FAST \cite{FAST}, MSER \cite{MSER}), vagy ezen pontokhoz tartozó leírók (\textit{feature descriptor}) kinyerésében (pl. FREAK \cite{FREAK}), vagy mindkettőre használhatóak (pl. ORB \cite{ORB}, SIFT \cite{SIFT} és SURF \cite{SURF}). SIFT és a SURF a tradícionális leírók közé sorolhatóak abban a tekintetben, hogy vektor-alapú leírokat készítenek, ellenben az újabbakkal, amelyek bináris-füzéreket. Előbbi algoritmusok kiszámolása idő- és erőforrásigényes, valós idejű, valamint mobil eszközökön történő alkalmazásra kevésbé alkalmasak, utóbbiak viszont igen, és a kinyert leírók összehasonlítása is gyorsabb (Hamming-távolság). Dolgozatomnak nem volt célja ezek mélyreható vizsgálata és rangsorolása, de Bekele és társai munkássága \cite{feature-detection-comparison} alapján először a FREAK-ket próbáltam meg alkalmazni több különböző forgatás- és skálainvariáns detektor által visszaadott kulcspontokra, sajnos kevés sikerrel. Valószínűleg nem sikerült megfelelő paraméterezést találnom, mivel nem kaptam jobb végeredményt. Végül az ORB-ot próbáltam ki, amivel már kielégítő eredményt kaptam, hasonlóan a SURF-fel végzett kísérletekhez, ellenben jóval kevesebb idő alatt, így ennél maradtam.

Miután mindkét képen megkerestem a jellegzetes pontokat, és kinyertem a hozzájuk tartozó leírókat, ezeket párosítanom kellett. Ehhez használhatunk brute-force módszert (mindent mindennel összehasonlítva és kiválasztva a legközelebbit), vagy a FLANN (Fast Approximate Nearest Neighbor Search Library \cite{flann_pami_2014}) könyvtárat, melyhez OpenCV-ben is elkészült egy interfész \cite{opencv-flann}. Ez első sorban arra használható, hogy gyorsan tudjunk több dimenziós vektortérben egy vektorhoz a hozzá legközelebbi vektort megkeresni, amely SIFT és SURF esetben ideális, de használható bináris leírókhoz is \cite{flann-binary}. Mivel a halmaz mérete, ahol a párosítást keressük, nem olyan nagy, hogy a brute-force módszer hátránya kiütközzön, ezért ezt választottam, melynek OpenCV-ben az implementációját a \texttt{cv::BFMatcher} osztályban találjuk.

Végül a kapott párosításokat a fundamentális mátrix segítségével validáltam és megszűrtem, vagyis megnéztem, hogy a már használt epipoláris ($\mathbf{u}'^T \mathbf{F} \mathbf{u} = 0$) kényszer egy adott hibahatáron belül teljesült-e. Megjegyzendő, hogy ettől még maradhatott teljesen rossz párosítás is a halmazban, hiszen ilyenkor az adott ponthoz csak azt ellenőrizzük, hogy a másik pont rajta van-e a hozzá tartozó epipoláris egyenesen. \Aref{fig:multi-obj-matches}. ábrán látható két kamera két képének a blobok által kijelölt részén a legközelebbi 20 találat. 

\begin{figure}[tbh]
\centering

\begin{tikzpicture}[spy using outlines={circle,white,magnification=3,size=1.5cm, connect spies}]

\node {\includegraphics[width=400pt]{figures/multi_obj_matches.png}};
\spy on (-5.33,-1.5) in node [left] at (-5.5,0.5);
\spy on (2.0,-1.58) in node [left] at (4.7,-1.7);

\spy on (-1.8,-0.1) in node [left] at (-2.8,1.7);
\spy on (5.4,-0.14) in node [left] at (4,1.7);

%\tikzset{help lines/.style={color=blue!50}}
%\draw[thick,step=1cm,help lines] (-7,-2) grid (7,2);

\end{tikzpicture}

\caption{Blobok által kijelölt képrészleteken az egymásnak megfelelő pontok keresése. A találatok szempontjából érdemi rész látható, a 20 legközelebbi találattal és ezekből 1-1 kiemelve közelről \label{fig:multi-obj-matches}}
\end{figure}

Ezt követően a blobokat a következőképpen párosítom. Veszem azt a képet, amelyiken több blob van, mint a másikon (ha ugyanannyi, akkor a választás tetszőleges), majd egyesével az itt lévő blobokhoz megkeresem, hogy a benne lévő párosításbeli pontok párjainak többségét melyik másik képen lévő blob tartalmazza. Így kapok egy projekciót a több elemű halmazból a kisebbe. Nem feltétlen lesz minden blobnak párja (nem tartalmaz \textit{feature} pontot), valamint lehet, hogy több különböző blob is ugyanahhoz a blobhoz fog tartozni, ekkor ezeket egyesítem. Így a végén a blobok egy részhalmazához egy kölcsönösen egyértelmű relációt, párosítást kapok, ezek jelölik ugyanazon objektum két maszkját a két képen. %\Aref{fig:multi-obj-matches} képrészleten látható objektumok esetén például a 367 talált párosításból a 1 objektumhoz tartozó maszkból atból összesen 30 volt két nem egymáshoz tartozó blobok között

A következő lépések könnyebb tesztelése végett aktívan használtam a csak egy objektum detektálására képes algoritmust is, ezért a két megoldást egymással konform módon valósítottam meg. Mindkettőt egy közös \texttt{ObjectSelector} absztrakt osztályból származtattam, lásd \ref{fig:cd:objectselector}. ábra. A továbbiakban fogjuk látni, hogy ennél a módszernél is jó, ha meghatározzuk az egymásnak megfelelő pontokat, így mindkettő megkapta konstruktorában a \texttt{Matcher} osztály egy példányát, melyet a \texttt{ObjectSelector} őriz. A \texttt{Matcher} két képkockából és az előtér maszkokból meghatározza az egymásnak megfelelő pontokat ORB-bal, melyeket pontpárok listájaként ad vissza. Konstruktorában azért, hogy a találatokat szűrni tudja, megkapja a kamera objektumokat és a fundamentális mátrixot. Mindkét konkrét \texttt{ObjectSelector} megvalósítás \texttt{Object} listával tér vissza, mely tartalmazza a két maszkot a két képen, valamint az összetartozó pontokat.


\begin{figure}[tbh]
\centering

\begin{tikzpicture} 

\begin{class}[text width=5.5 cm]{Matcher}{4.5, 0}

\operation{+ Matcher(cam1, cam2, F)}
\operation{+ match(imgs, masks) : ~~~~~~~ vector<pair<Point2f, Point2f>{}>}

\end{class}

\begin{abstractclass}[text width=7 cm]{ObjectSelector}{-3.8, 0}

\operation   {+ ObjectSelector(matcher)}
\operation[0]{+ select(frames, masks) : vector<Object>}

\end{abstractclass}


\aggregation{ObjectSelector}{matcher~~~~~~~~~~}{1}{Matcher}


\begin{class}[text width=7 cm]{SingleObjectSelector}{-3.8, -3}
\inherit{ObjectSelector}

\operation{+ SingleObjectSelector(matcher)}
\operation{+ select(frames, masks) : vector<Object>}
\end{class}

\begin{class}[text width=7 cm]{MultiObjectSelector}{4, -3}
\inherit{ObjectSelector}

\operation{+ MultiObjectSelector(matcher)}
\operation{+ select(frames, masks) : vector<Object>}
\end{class}

\begin{class}[text width=7.5 cm]{Object}{0, -6}
\operation{+ masks : vector<Mat>}
\operation{+ matches : vector<pair<Point2f, Point2f>{}>}
\end{class}

\draw[umlcd style dashed line,->] (SingleObjectSelector.south) ++
(-3,0) |- node [above,sloped,black,pos=0.75]{$ < <$ instantiate $ > >$} (Object);

\draw[umlcd style dashed line,->] (MultiObjectSelector.south) ++
(3,0) |- node [above,sloped,black,pos=0.75]{$ < <$ instantiate $ > >$} (Object);

\end{tikzpicture}

\caption{Objektumok kijelölését szolgáló osztályok \label{fig:cd:objectselector}}
\end{figure}



%----------------------------------------------------------------------------
\section{Optikai folyam meghatározása}
%----------------------------------------------------------------------------

\Aref{sec:of-mask}. szekcióban bemutattam, hogyan lehet felhasználni a sűrű optika folyamokat előtér maszk meghatározására, a következőkben pedig azt írom le, hogyan lehet segítségükkel a két kamera egy objektumhoz tartozó két képén sűrű pontmegfeleltetést meghatározni.

Előzőekben már ismertettem a \texttt{cv::calcOpticalFlowFarneback()} függvényt, és annak néhány paraméterét, amelyeket úgy állítottam be, hogy a lehető leggyorsabban kapjak még használható becslést a mozgásról. Most viszont fontos, hogy a kapott eredmény pontosabb legyen, tehát ezeken kissé változtatnom kellett. Ezért 6 szintes piramist (\texttt{levels = 6}), és köztük 0,75-ös skálázást (\texttt{pyr\_scale = 0.75}) állítottam be, hogy a nagyobb elmozdulásokat is detektálni tudjam. A Gauss-zaj feltételezése miatt bekapcsoltam, hogy egyszerű dobozszűrő helyett Gauss-szűrőt használjon az algoritmus, és az ehhez kapcsolódó paramétereket a dokumentációban adott javaslatok alapján állítottam be, valamint emiatt az ablakméretet is növeltem (\texttt{winsize = 21}). A futási idő természetesen jelentősen függ a kép méretétől, így fontos, hogy kihasználjam az objektum maszkjából nyerhető információt.

A következőkben \aref{fig:of_original}. ábrán látható két képkocka lesz a kiinduló állapot, már az előbbiekben bemutatott objektum detektálás után, az előtér maszk által kijelölve. A képkockák két olyan kamera beállításal készültek, ahol a két kamera képsíkja nagyjából egybe estek (egy irányba néztek), és csak víszintes irányban voltak egymáshoz képest eltolva.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.49\linewidth}
	\centering
	
	\begin{tikzpicture}

%left: 0.315625 0.225 0.667188 0.9
%right: 0.034375 0.208333 0.471875 0.889583
%uni: 0.034375 0.208333 0.667188 0.9

\node[anchor=north west,inner sep=0] (image) at (0,0) {\includegraphics[width=205pt]{figures/of_img_left.png}};

\begin{scope}[x={(image.north east)},y={(image.south west)}]
    \draw[very thick,yellow] (0.315625, 0.225) rectangle (0.667188, 0.9);
    \draw[very thick,dashed,red] (0.034375, 0.208333) rectangle (0.667188, 0.9);
\end{scope}

\end{tikzpicture}

  \end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\begin{tikzpicture}

%left: 0.315625 0.225 0.667188 0.9
%right: 0.034375 0.208333 0.471875 0.889583
%uni: 0.034375 0.208333 0.667188 0.9

\node[anchor=north west,inner sep=0] (image) at (0,0) {\includegraphics[width=205pt]{figures/of_img_right.png}};

\begin{scope}[x={(image.north east)},y={(image.south west)}]
    \draw[very thick,yellow] (0.034375, 0.208333) rectangle (0.471875, 0.889583);
    \draw[very thick,dashed,red] (0.034375, 0.208333) rectangle (0.667188, 0.9);
\end{scope}

\end{tikzpicture}
  \end{subfigure}
\caption{Bal és jobb kamera által látott objektum kijelölve; a sárga keretek jelölik az objektumok befoglaló téglalapjait, a piros szaggatott pedig ezen téglalapokat tartalmazó legkisebb területű téglalapot \label{fig:of_original}}
\end{figure}

Első megközelítésem, hogy a két objektum befoglaló téglalapját tekintettem, és vettem azt a téglalapot, mely a legkisebb területű azok közül, amely mindkettőt tartalmazza. Kivágva ezt a téglalapot a két képből, két egyforma méretű képrészletet kaptam, melyek külön-külön tartalmazzák a teljes objektumot. Ez látható \aref{fig:of_original}. ábrán pirossal jelölve. Erre a két részletre számolva optikai folyamot, 7 646 darab vektort kaptam, melyek közül néhányat vizualizáltam \aref{fig:bad0}. ábrán. A vektorok kezdő és végpontjaiból alkottam pontpárokat, ezeket tekintettem egymásnak megfelelő pontoknak a két képen. Jól látható, hogy a kevés kirajzolt pontpárból egyik sem jó, mert a doboz különböző lapjaihoz tartoznak. Ez a nagy elmozdulás miatt van, hiába a piramis módszer, használhatatlan a végeredmény.

\begin{figure}[tbh]
\centering
\includegraphics[width=300pt]{figures/vis_bad_0.png}
\caption{Első megközelítés (36 958 vektor) \label{fig:bad0}}
\end{figure}

A következő lépésem, hogy meghatároztam azt a vektort, amivel eltolva az egyik képet, az egymásnak megfelelő képpontok elmozdulásai a lehető legkisebbek lettek. Ehhez először szükségem volt néhány egymásnak megfelelő pontpárra.

Az objektumok meghatározásánál (legyen az egy vagy több), már meghatároztam néhány egymásnak megfelelő pontot, így ezeket itt újra fel tudtam használni (\texttt{Object::matches}). A párosítások jelentette vektorokból kellett meghatároznom azt a vektort, amellyel az egyik képet eltolva az összetartozó pontok közötti távolság minimális. Azaz

\[\argmin_{\mathbf{v}(x, y)} \sum_{i=1}^{n} |\mathbf{v} - \mathbf{d_i}|\]

ahol $\mathbf{d_1}(x_1, y_1), \mathbf{d_2}(x_2, y_2), \ldots \mathbf{d_n}(x_n, y_n)$ a párosításban szereplő pontok közti távolságvektorok. A problémát úgy is megfogalmazhatjuk, hogy a $\mathbf{d_i}$ vektorokat pontoknak tekintjük és $\mathbf{v}$-t, a pontok geometriai mediánját keressük. Bizonyították \cite{Bajaj198699}, hogy ennek megoldásához nem létezik konkrét formula, vagy algoritmus csak aritmetikai műveletek és k. gyök felhasználásával. Ellenben konvex függvényről lévén szó, iteratíve közelíthetünk a megoldáshoz. Ezt elkerülendő, a távolságok összege helyett közelítő megoldásként a távolság-négyzetösszegét minimalizáltam, tehát:

\[\argmin_{(x, y)} \sum_{i=1}^{n} \Big((x-x_i)^2 + (y-y_i)^2\Big)\]

Ismeretes viszont, hogy a fenti a minimumhelyét az $(x_i, y_i)$ pontok súlypontjában veszi fel. \Aref{fig:shifted}. ábrán látható az eltolás előtt és után az objektum helyzete a két képen.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[height=102pt]{figures/before_shift_top.png}\\\vspace{3pt}
	\includegraphics[height=102pt]{figures/before_shift_bottom.png}
  \end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[height=102pt]{figures/after_shift_top.png}\\\vspace{3pt}
	\includegraphics[height=102pt]{figures/after_shift_bottom.png}
  \end{subfigure}
\caption{Az objektum elhelyezkedése a kamerák képein (egymás alatt a két kamera képe) eredetileg és a kiszámolt vektorral való eltolás után. \label{fig:shifted}}
\end{figure}

Optikai folyamok számítása esetén két jelenséget nem szabad figyelmen kívül hagynunk: egyik a textúrázatlanság, a másik pedig a kitakart pontok problémája. Ezeken az optikai folyam rektifikációja segíthet. \cite{optical-flow-rectification}-ban You Yang és társai egy bináris függvényt javasolnak a textúrázatlanság eldöntésére:

\[
    \zeta(\Omega_\mathbf{X})= 
\begin{cases}
    0,              & \text{ha } \sigma(I_{\forall \mathbf{Y}\in\Omega_\mathbf{X}} - I_\mathbf{X}) < \varepsilon_\Omega\\
    1,              & \text{különben}
\end{cases}
\]

ahol $\Omega_\mathbf{X}$ jelöli $\mathbf{X}$ képpont egy környezetét, $I_\mathbf{X}$ az $\mathbf{X}$ pont intenzitását, $\sigma(.)$ a szórás-operátort egy halmazra nézve, valamint $\varepsilon_\Omega$ egy küszöbértéket, ami konstans. Úgy találták, hogy $\varepsilon_\Omega = 6$ választással jó eredményeket értek el, így én is ezt használtam. A mintaképek pontjaira kiszámolva ezt a függvényt, \aref{fig:textures}. ábrán látható maszkokat kaptam.

\begin{figure}[tbh]
\centering
\includegraphics[width=300pt]{figures/textures.png}
\caption{$\zeta$ függvény alkalmazva az objektum összes pontjára \label{fig:textures}}
\end{figure}

A kitakart pontok kiszűrésére én Yang-ék megoldásától \cite{optical-flow-rectification} eltérő megközelítést alkalmaztam. Legyen két képkocka $K_1$ és $K_2$, valamint $F_{1, 2} = \mathcal{F}(K_1, K_2)$ és $F_{2, 1} = \mathcal{F}(K_2, K_1)$, ahol $\mathcal{F}$ jelöli két képkocka közti optikai folyam operátort, melynek eredménye egy vektormező ($F_{1, 2}, F_{2, 1} : \mathbb{R}^2 \rightarrow \mathbb{R}^2$). Gondoljuk meg, hogy ha $x\in K_1$ és $x + F_{1,2}(x) = x' \in K_2$, akkor $x' + F_{1,2}(x') \approx x \in K_1$, vagyis ha egy $x$ pont $K_1$-ről $K_2$-re az $x'$ pontba mozog, akkor visszafelé nézve $x'$ pontnak ideális esetben $x$ pontba kell mozognia. Tehát oda-vissza számolva 1-1 optikai folyamot a kitakart pontokat kiszűrhetjük, hiszen a másik irányban nem fogjuk megtalálni a párosítást.

A fent leírtakat az \texttt{OpticalFlowCalculator} osztályban implementáltam (rövid áttekintő látható \aref{fig:cd:ofcalc}. ábrán). Egy publikus metódusa van \texttt{calcDenseMatches()} néven, mely megkapja a két képkockát és egy objektumot, amelyre a sűrű pontmegfeleltetést szeretnénk számolni. Ez lényegében három dolgot csinál: meghatározza a textúrázott régiókat (\texttt{calcTexturedRegion}) a két képen, kiszámolja oda és vissza az optikai folyamokat (\texttt{calcOpticalFlows}), majd ezek alapján párosítja a pontokat (\texttt{collectMatchingPoints}).

\begin{figure}[tbh]
\centering

\begin{tikzpicture} 

\begin{class}[text width=8 cm]{OpticalFlowCalculator}{0, 0}

\attribute{- frames : Mat[]}
\attribute{- masks : Mat[]}
\attribute{- texturedRegions : Mat[]}

\operation{- calcTexturedRegion(frame, mask) : Mat}
\operation{- calcOpticalFlows() : vector<Mat>}
\operation{- collectMatchingPoints(flows) : pair<vector<Point2f>, vector<Point2f>{}>}

\operation{+ calcDenseMatches(frames, object) : pair<vector<Point2f>, vector<Point2f>{}>}

\end{class}

\end{tikzpicture}

\caption{Sűrű pontmegfeleltetések optikai folyamok segítségével meghatározó \texttt{OpticalFlowCalculator} osztály \label{fig:cd:ofcalc}}
\end{figure}

Az algoritmust lefuttatva egész pontos párosításokat kaptam, \aref{fig:vis_full}. ábra mutat be ezek közül néhányat. Összesen 16 841 pár jött létre, tehát ennyit tudtam felhasználni a háromszögeléshez. %Vessük össze ezt a SURF leírókka történő párosításnál nyert 100, majd abból meghagyott 25 párral, számottevő a különbség.

\begin{figure}[tbh]
\centering
\includegraphics[width=300pt]{figures/vis_full.png}
\caption{Végső párosítás az optikai folyamok segítségével (16 841 vektor) \label{fig:vis_full}}
\end{figure}

%----------------------------------------------------------------------------
\section{Háromszögelés}
%----------------------------------------------------------------------------

Ahogy \aref{sec:triangulation}. alfejezetben bemutattam az elméleti hátteret, a következőkben ennek implementálását tárgyalom.

\subsection{OpenCV-s függvényekkel}

OpenCV \texttt{cv::triangulatePoints} függvényben a \textit{Linear-LS} (lineáris legkisebb négyzetek) módszer van implementálva. A dokumentáció \cite{camera-calib-3d} alapján sztereó-kalibráció során nyert projekciós mátrixokat vár a pontpárok mellett paraméterül. Az én esetemben a kamerák nincsenek sztereó-kalibrálva, de a két projekciós mátrix megvan. Ennek ellenére a kimeneti 3D-s koordináták használhatatlanok voltak. Kis kutatás után az \texttt{cv::undistortPoints()} függvény meghívása jelentette a megoldást. Ez először normalizálta őket, vagyis:

\[\left[\begin{array}{c} u' \\ v' \\ 1 \end{array}\right] = {\underbrace{\left[\begin{array}{ccc}
f_x & 0 & o_x \\ 
0 & f_y & o_y \\
0 & 0 & 1
\end{array}\right]}_{\hbox{kamera-mátrix}}}^{-1} \left[\begin{array}{c} u \\ v \\ 1 \end{array}\right]\]

ahol $(u', v')$ az $(u, v)$ pont normalizáltja (kamera-mátrixtól független koordináták), majd a torzítási együtthatóakat felhasználva az $(u', v')$ pontoknak meghatározta a javított koordinátájukat. Ezután a \texttt{cv::triangulatePoints} függvénynek a projekciós mátrixok helyett csak az $\Big(\,\mathbf{R}\,|\,\mathbf{t}\,\Big)$ mátrixokat kellett átadnom. Fontos, hogy az előbbiekben kialakított pontpárosítás azon osztályához tartozó pontokat, melyek az eltolt képhez tartoznak, az eltoláshoz használt vektor ellentettjével vissza kellett tolni, hogy valós koordinátákat kapjak.

Miután meghatároztam a pontok valóvilágbeli koordinátáikat, ezek pontosságát a képsíkokra történő visszavetítéssel vizsgáltam meg. A pontokat a \texttt{cv::projectPoints} függvénnyel vetítettem a bal és jobb oldali kamera képére, majd ezek és az eredeti pontok távolságainak átlagát -- átlagos visszavetítési hiba -- vizsgáltam. Ez az előzőekben mutatott bemenetre 8,68716 pixel lett, amit elfogadható hibának vettem, hiszen azt jelenti, hogy minden pont átlagosan egy 3 pixel sugarú körön belül csúszott el. \Aref{fig:cv-triangulation}. ábrán látható az eredményeket bemutató két vizualizáció, melyeken a szín a pontok $z$ koordinátáját jelzi. 

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/visu_left.png}
	\caption{Bal oldali kamera nézőpontjából nézve \label{fig:cv-triangulation-a}}
  \end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/visu_pcl.png}
	\caption{PCL vizualizációs szoftverrel ráközelítve}
  \end{subfigure}
\caption{Háromszögelés \textit{Linear-LS}-sel. Jól látható a közeli nézőpontból, hogy egy kissé hullámos lett a felület \label{fig:cv-triangulation}}
\end{figure}

Az OpenCV keretrendszerben megtalálhatjuk a \cite{hartley-triangulation}-ben leírt optimális, de polinomiális algoritmust is implementálva \texttt{cv::correctMatches()} néven, amely a fundamentális mátrix segítségével \aref{sec:triangulation}. szakaszban említett módon javítja a pontpárokat, azaz $\mathbf{u} \leftrightarrow \mathbf{u}'$ összerendelések helyett olyan $\mathbf{\hat{u}} \leftrightarrow \mathbf{\hat{u}}'$ párokat ad, melyekre $d(\mathbf{u}, \mathbf{\hat{u}})^2 + d(\mathbf{u}', \mathbf{\hat{u}}')^2$ minimális, és teljesül, hogy $\mathbf{\hat{u}}'^T \mathbf{F} \mathbf{\hat{u}} = 0$. Ez az előbbi átlagos visszavetítési hibát 8,054 pixelre javította, de rohamos teljesítménycsökkenés mellett (0,06 másodperces futási idő helyett 0,64 másodperc lett a háromszögelés), mely az eredményeket szabad szemmel megnézve nem volt meggyőző.


\subsection{Iteratív lineáris legkisebb négyzetek (\textit{Iterative-LS})}

\Aref{sec:triangulation}. szakaszban leírt iteratív módszert nem tudjuk alkalmazni a fenti \texttt{cv::triangulatePoints} függvénnyel, mert nem lehet az egyenleteket pontonként külön-külön súlyozni. Ennek megfelelően először a ,,szimpla'' \textit{Linear-LS} megközelítést kellett implementálnom, majd ezt már meghívhattam iteratívan különböző súlyokkal.

Ehhez én Roy Shilkrot online elérhető alkalmazás-könyvtárából \cite{sfm-toy-library} merítettem a kiindulási alapot, és azt alakítottam az általam használt adatszerkezetekhez. Ehhez is előtte minden pontot normalizálni, valamint a torzítási együtthatóknak megfelelően javítani kellett. Ezzel a megközelítéssel 8,21 pixelnyi átlagos visszavetítési hibát kaptam, de a sebesség harmadára csökkent (0,06 másodperc helyett 0,18 másodperc lett a futási idő). % melyet szintén nem tekinthetünk kifizetődő kompromisszumnak.

A fentieket összegyűjtve a \texttt{Triangulator} osztályban (lásd \ref{fig:cd:triangulator}. ábra) implementáltam, melynek két metódusa a fent említett két módszert (OpenCV-s \texttt{triangulatePoints}, valamint az Iterative-LS) valósítja meg. Mindkettő az egymásnak megfelelő pontpárokat várja bemenetként, és harmadik paraméterében visszaadja a pontfelhőt, mely \texttt{CloudPoint}-ok listájaként áll elő. Egy \texttt{CloudPoint} az aktuális koordinátákon kívül azt is tudja magáról, hogy mekkora a hozzá tartozó visszavetítési hiba, így a megjelenítésnél ezt is figyelembe tudtam venni.

\begin{figure}[tbh]
\centering

\begin{tikzpicture} 

\begin{class}[text width=9.75 cm]{Triangulator}{0, 0}
\attribute{+ camera1 : Camera}
\attribute{+ camera2 : Camera}

\attribute{+ cameraPose1 : CameraPose}
\attribute{+ cameraPose2 : CameraPose}

\operation{+ triangulateIteratively(points1, points2, cloudpoints) : double}
\operation{+ triangulateCv(points1, points2, cloudpoints) : double}
\end{class}

\begin{class}[text width=3.5 cm]{CloudPoint}{7.75, 0}

\attribute{+ pt : Point3d}
\attribute{+ reprojErr : double}

\end{class}

\end{tikzpicture}

\caption{\texttt{Triangulator} osztály és a \texttt{CloudPoint} struktúra \label{fig:cd:triangulator}}
\end{figure}

%----------------------------------------------------------------------------
\section{Változtatható nézőpont}
%----------------------------------------------------------------------------

Ahogy az előbbiekben (lásd \ref{fig:cv-triangulation-a}. ábra) már említettem, a \texttt{cv::projectPoints} függvény segítségével lehet meghatározni, hogy adott kamerából fényképezve a valóvilágbeli pontoknak hova esnek a vetületei. Háromféle megjelenítési technikát implementáltam; \begin{inparaenum}[\itshape 1\upshape)]
\item $z$ koordinátát jelölő színkód,
\item eredeti képpontok színeinek felhasználása, valamint a
\item kontúrok rajzolása.
\end{inparaenum}

Az első implementálását a HSV (hue, saturation, value) színtér segítségével valósítottam meg. $S$ és $V$ értékét fixen a maximumra állítottam, $H$ értékét pedig egyenletesen elosztottam a különböző $z$ koordináták mentén.

A másodikhoz a pontfelhő mellé még két információra volt szükség: a pontok koordinátáira a bal képen, valamint magára a bal képre, hogy a pixelek színeit ki tudjam nyerni. Ezek után az adott 3D-s pont vetített képpontjának a színét a bal képen lévő forrás pixel színére állítottam.

A harmadik megoldást 3 lépésben valósítottam meg. Először fehér pontokként levetítettem a pontokat a képsíkra. Ezt követően dilatáció-erózió kombinációval morfológiai zárást hajtottam végre a bináris képen, minek köszönhetően a kisebb lyukak és szakadások megszűntek. Végül az eredményen kontúrokat kerestem a \texttt{cv::findContours()} függvény segítségével, és ezeket kirajzoltam. Egy határérték (amit 100 pixel$^2$-nek választottam kézi hangolás után) alatti területtel rendelkező kontúrokat elvetettem, mert ezek olyan kisebb foltokat jelentenek, melyeket nem tudunk egyértelműen egy nagyobb objektumhoz rendelni.

Ezen három vizualizáció eredménye a bal oldali kamera nézpontjából látható \aref{fig:different-vis}. ábrán.

\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.33\linewidth}
	\centering
	\includegraphics[width=135pt]{figures/visu_depth.png}
	\caption{Mélységinformáció}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=135pt]{figures/visu_pixels.png}
	\caption{Eredeti pixelek}
  \end{subfigure}
\begin{subfigure}[b]{.32\linewidth}
	\centering
	\includegraphics[width=135pt]{figures/visu_contours.png}
	\caption{Kontúrok}
  \end{subfigure}
\caption{Különböző vizualizációk, ráközelítve a hasznos területre \label{fig:different-vis}}
\end{figure}

Ahogy \aref{chapter3}. fejezetben említettem, a választott nézőpont, ahonnan érdemes lehet rekonstruálni az objektumot, a két kamera nézőpont között helyezkedhet el. Tehát a két kamera között kellett interpolálni a virtuális kamera helyzetét és irányát, és innen elvégezni a vetítést. Fontos, hogy nem csak a kamera helyzetét ($\mathbf{t_v}$), a forgatási mátrixát ($\mathbf{R_v}$) is meg kellett határoznom. Jelölje $r \in [0; 1]$ a virtuális kamera pozícióját a pályáján, ahol $r = 0$, ha a bal oldali kamera, és $r = 1$, ha a jobb oldali kamera helyzetében van. Ekkor a virtuális kamera eltolási vektora $\mathbf{t_v} = (1-r)\mathbf{t_b} + r\mathbf{t_j}$, ahol $\mathbf{t_b}$ jelöli a bal oldali, $\mathbf{t_j}$ a jobb oldali kamera eltolási vektorát. Ken Shoemake cikke \cite{quaternion} alapján célszerű a forgatási mátrixok (vagy akár az ebből nyerhető Euler-szögek) helyett kvanterniókra áttérni, és ezek között úgynevezett gömbi lineáris interpolációval (\textit{slerp}) kiszámolni a kívánt köztes lépést. Ennek implementációját az Eigen \cite{eigenweb} alkalmazáskönyvtárral valósítottam meg. Először az OpenCV-s \texttt{cv::Mat} mátrix objektumokat Eigen-es \texttt{Eigen::Matrix} objektumokká konvertáltam, majd ezeket már az API-t használva kvaterniókká alakítva a \texttt{slerp()} metódus segítségével egy köztes kvaterniót interpoláltam. Végül ezt visszaalakítottam $\mathbf{R_v}$ OpenCV-s mátrixszá.

Ezt a logikát a \texttt{Visualization} osztályban implementáltam, mely konstruktorában egy \texttt{CameraPose} példányt (eltolási és forgatási vektor -- lásd Rodrigues-formula), valamint egy kamera-mátrixot vár. Négy további metódusa van, ebből három a fenti megjelenítési típusokat valósítja meg, az utolsó pedig visszaadja magát az elkészült képet. Az áttekintő osztálydiagramot \aref{fig:cd:visualization}. ábra mutatja be. Az $r$ arányszámot a felületről egy csúszka segítségével tudjuk változtatni, \aref{fig:visu_slider}. ábra (lásd \pageref{fig:visu_slider}. oldal) mutatja a két szélső és a középső helyzetben a vizualizáció eredményét.

\begin{figure}[tbh]
\centering

\begin{tikzpicture} 

\begin{class}[text width=9.5 cm]{Visualization}{0, 0}
\attribute{- cameraPose : CameraPose}
\attribute{- cameraMatrix : Mat}
\attribute{- result : Mat}

\operation{+ Visualization(cameraPose, cameraMatrix)}

\operation{+ renderWithDepth(points) : void}
\operation{+ renderWithContours(points) : void}
\operation{+ renderWithColors(points, imgPoints, img) : void}

\operation{+ getResult() : Mat}

\end{class}

\end{tikzpicture}

\caption{Vizualizációs osztály \label{fig:cd:visualization}}
\end{figure}


\begin{figure}[tbh]
\centering
\begin{subfigure}[b]{.43\linewidth}
	\centering
	\includegraphics[width=175pt]{figures/of_img_left.png}\\\vspace{10pt}
	\includegraphics[width=175pt]{figures/visu_pixels_left.png}
	\caption{Bal oldali kamera képe és a helyre\-ál\-lí\-tás ugyanebből a nézőpontból}
  \end{subfigure}\hspace{30pt}
\begin{subfigure}[b]{.43\linewidth}
	\centering
	\includegraphics[width=175pt]{figures/of_img_right.png}\\\vspace{10pt}
	\includegraphics[width=175pt]{figures/visu_pixels_right.png}
	\caption{Jobb oldali kamera képe és a helyre\-ál\-lí\-tás ugyanebből a nézőpontból}
  \end{subfigure}\\\vspace{15pt}
\begin{subfigure}[b]{.60\linewidth}
	\centering
	\includegraphics[width=205pt]{figures/visu_pixels_center.png}
	\caption{Helyreállítás a két kamera közötti, interpolált nézőpontból}
  \end{subfigure}
\caption{Választott nézőpont \inlinelist{\protect\item bal kamera, \protect\item a jobb kamera és \protect\item a két kamera közötti pozícióban} \label{fig:visu_slider}}
\end{figure}

%----------------------------------------------------------------------------
\section{Teljes folyamat együtt, implementációk kiválasztása}
%----------------------------------------------------------------------------

Az előző szekciókban felvázolt lépéseknél három helyen készítettem két különböző implementációt, ezek között az alábbiak szerint döntöttem. Az előtér maszk meghatározásához végül az \texttt{OFForegroundMaskCalculator}t választottam. Ugyan tovább tart, mint a párja, viszont sokkal pontosabb maszkot eredményez, ami jobb rekonstrukciót tesz lehetővé. A \texttt{MOG2ForegroundMaskCalculator} által kiszámolt előterek nagyon hiányosak voltak, és nem sikerült a paraméterezését úgy módosítanom, hogy ez javuljon. Az objektum-detektáláshoz természetesen a \texttt{MultiObjectSelector}-t választottam, hiszen a másik csak tesztelés miatt került implementálásra, hogy két képkockáról gyorsan kapjak egy objektumot, amire a további lépéseket végrehajthatom. A \texttt{Triangulator} osztály esetén pedig az OpenCV-s metódus mellett döntöttem (\texttt{triangulateCv}), mert csak kicsit pontatlanabb, ellenben természetéből adódóan jóval gyorsabb (átlagosan 5-ször), mint az iteratív megoldás.

Egy képkockapár feldolgozásának a folyamata szekvencia-diagramként látható \aref{fig:sd:objects}. és \aref{fig:sd:tria}. ábrákon. Az első mutatja be a képkockák lekérésétől az objektumok meghatározásáig a folyamatot, a második pedig az objektumok rekonstruálását és vizualizációját.

\begin{figure}[tbh]
\centering

\resizebox{0.9\textwidth}{!}{

\begin{sequencediagram}
\newthread{m}{: main}

\newinst[0.5]{cam}{: Camera}
\newinst[0.5]{maskCalc}{: F.MaskCalculator}

\newinst{objSelector}{: ObjectSelector}
\newinst[1.2]{matcher}{: Matcher}


\begin{sdblock}{Kamera ciklus}{mindkét kamerára}

    \begin{call}{m}{read(frame)}{cam}{frame}
    \end{call}
    
    \begin{call}{m}{calculate(frame)}{maskCalc}{mask}
    \end{call}

\end{sdblock}

\begin{call}{m}{selectObjects(frames, masks)}{objSelector}{objects}
    
    \begin{call}{objSelector}{blobok létrehozása}{objSelector}{}
    \end{call}
    
    \begin{call}{objSelector}{match(frames, newMasks)}{matcher}{matches}
    \end{call}

    \begin{call}{objSelector}{blobok párosítása}{objSelector}{}
    \end{call}

\end{call}

\end{sequencediagram}
}

\caption{Egy képkocka-pár esetén az objektumok kijelöléséhez szükséges lépések szekvencia diagramja \label{fig:sd:objects}}\vspace{25pt}

\begin{sequencediagram}

\tikzstyle{every node}=[font=\small]

\newthread{m}{: main}

\newinst[1]{of}{: OpticalFlowCalculator}
\newinst{triang}{: Triangulator}
\newinst{vis}{: Visualization}


\begin{sdblock}{Objektum ciklus}{object : objects}

    \begin{call}{m}{calcDenseMatches(frames, object)}{of}{(pts1, pts2)}
    
        \begin{call}{of}{calcTexturedRegion(frame, mask)}{of}{}\end{call}

        \begin{call}{of}{calcOpticalFlows()}{of}{flows}\end{call}
        
        \begin{call}{of}{collectMatchingPoints(flows)}{of}{(pts1, pts2)}\end{call}
    
    \end{call}
    
    \begin{call}{m}{triangulateCv(pts1, pts2, pcloud)}{triang}{pcloud}
    \end{call}

    \begin{call}{m}{pontfelhők összevonása}{m}{}
    \end{call}

\end{sdblock}


\begin{call}{m}{renderWithDepth(pointcloud)}{vis}{}
\end{call}
    
\end{sequencediagram}

\caption{Objektumonkénti rekonstrukció és vizualizáció szekvencia diagramja \label{fig:sd:tria}}

\end{figure}



%----------------------------------------------------------------------------
\section{Összefoglaló}
%----------------------------------------------------------------------------

Ebben a fejezetben az elkészült alkalmazás implementációjának fejlesztését mutattam be. Az egyes fázisokat, részproblémák megoldásait példákon keresztül szemléltettem. Amennyiben egy problémára több megoldást is elkészítettem, indokoltam a választásom valamelyik megközelítés mellett. Végezetül a konkrét alkalmazás futását szekvencia diagramon ábrázoltam.
